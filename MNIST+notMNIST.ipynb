{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "np.random.seed(1)\n",
    "rn.seed(2)\n",
    "tf.random.set_seed(3)\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, accuracy_score\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Add, Multiply, Lambda\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "N_hess=1000\n",
    "\n",
    "t_epoch=100\n",
    "t_batch=500\n",
    "\n",
    "d=784*2\n",
    "num_of_classes=100\n",
    "\n",
    "n_hl=2 # Number of hidden layers in teacher model\n",
    "hlu_teacher=500\n",
    "\n",
    "# Superfeatures\n",
    "M=2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Verbose\n",
    "train_verbose = 1\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# FUNCTION DEFINITION\n",
    "###############################################################################\n",
    "def set_seed_TF2(seed):\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    \n",
    "def model_nn_teacher(d, num_of_classes):\n",
    "    set_seed_TF2(100)\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(hlu_teacher, activation='relu', input_shape=(d,)))\n",
    "    for _ in range(n_hl-1):\n",
    "        nn.add(Dense(hlu_teacher, activation='relu'))\n",
    "    nn.add(Dense(num_of_classes))\n",
    "    nn.add(Activation('softmax'))\n",
    "    nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return nn\n",
    "\n",
    "def model_exp_teacher(d, num_of_classes, sf_idx):\n",
    "    set_seed_TF2(100)\n",
    "    inp=Input(shape=(d,))\n",
    "    logits=[]\n",
    "    for i in range(M):\n",
    "        x=Dense(hlu_new_teacher, activation='relu')(tf.gather(inp, sf_idx[i], axis=1))\n",
    "        for _ in range(n_hl-1):\n",
    "            x=Dense(hlu_new_teacher, activation='relu')(x)\n",
    "        y=Dense(num_of_classes, activation='softmax', name='sf_'+str(i+1))(x)\n",
    "        logits.append(tf.math.log(y+1E-15))\n",
    "    tot_logit=Add()(logits)-(M-1)*base_logit\n",
    "    op=Activation('softmax')(tot_logit)\n",
    "    nn = Model(inputs=inp, outputs=op)\n",
    "    nn.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return nn\n",
    "\n",
    "def solve(a, b, c):\n",
    "    if(b*b-4*a*c)<0:\n",
    "        raise ValueError('Problem is not feasible.')\n",
    "    sol=np.max(np.roots([a,b,c]))\n",
    "    return int(sol)\n",
    "\n",
    "def confidence_interval(a,l):\n",
    "    import numpy as np, scipy.stats as st\n",
    "    return st.t.interval(l, len(a)-1, loc=np.mean(a), scale=st.sem(a))\n",
    "\n",
    "def bootstrap_score(y_test, y_pred, metric=accuracy_score, l=0.95, seed=100):\n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    idx = np.arange(y_test.shape[0])\n",
    "    test_accuracies = []\n",
    "    for i in range(200):\n",
    "        pred_idx = rng.choice(idx, size=idx.shape[0], replace=True)\n",
    "        acc_test_boot = metric(y_test[pred_idx], y_pred[pred_idx])\n",
    "        test_accuracies.append(acc_test_boot)\n",
    "    bootstrap_score_mean = np.mean(test_accuracies)\n",
    "    [ci_lower, ci_upper] = confidence_interval(test_accuracies,l)\n",
    "    return bootstrap_score_mean, 0.5*(ci_upper-ci_lower)\n",
    "\n",
    "###############################################################################\n",
    "# EXPERIMENT\n",
    "###############################################################################\n",
    "\n",
    "# Dataset Preprocessing\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train_1, y_train_1), (x_test_1, y_test_1)=mnist.load_data()\n",
    "\n",
    "x_train_1=x_train_1.astype(np.float32).reshape(-1,784)\n",
    "x_test_1=x_test_1.astype(np.float32).reshape(-1,784)\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train_2, y_train_2), (x_test_2, y_test_2)=fashion_mnist.load_data()\n",
    "\n",
    "x_train_2=x_train_2.astype(np.float32).reshape(-1,784)\n",
    "x_test_2=x_test_2.astype(np.float32).reshape(-1,784)\n",
    "\n",
    "\n",
    "x_train=np.concatenate([x_train_1,x_train_2], axis=1)\n",
    "x_test=np.concatenate([x_test_1,x_test_2], axis=1)\n",
    "alpha_le=LabelEncoder()\n",
    "alpha_le.fit(['A','B','C','D','E','F','G','H','I','J'])\n",
    "alpha_train_2=alpha_le.inverse_transform(y_train_2)\n",
    "alpha_test_2=alpha_le.inverse_transform(y_test_2)\n",
    "y_train=[a+b for a,b in zip(y_train_1.astype(str),alpha_train_2)]\n",
    "y_test=[a+b for a,b in zip(y_test_1.astype(str),alpha_test_2)]\n",
    "\n",
    "le=LabelEncoder()\n",
    "y_train=le.fit_transform(y_train)\n",
    "y_test=le.transform(y_test)\n",
    "\n",
    "# Scaling\n",
    "#scl=StandardScaler()\n",
    "#x_train=scl.fit_transform(x_train)\n",
    "#x_test=scl.transform(x_test)\n",
    "x_train=x_train/255.0\n",
    "x_test=x_test/255.0\n",
    "\n",
    "y_train=y_train.reshape(-1,1)\n",
    "y_test=y_test.reshape(-1,1)\n",
    "\n",
    "\n",
    "# One-hot Encoding\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y_train=enc.fit_transform(y_train)\n",
    "\n",
    "idx=np.random.permutation(x_train.shape[0])[0:N_hess]\n",
    "x_hess=x_train[idx]\n",
    "###############################################################################\n",
    "# TEACHER\n",
    "###############################################################################\n",
    "teacher=model_nn_teacher(d,num_of_classes)\n",
    "teacher.fit(x_train, y_train, epochs=t_epoch, batch_size=t_batch, verbose=train_verbose)\n",
    "    \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print('Priviledged Test Classification')\n",
    "print(confusion_matrix(y_test, np.argmax(teacher.predict(x_test),1)))\n",
    "print(bootstrap_score(y_test, np.argmax(teacher.predict(x_test),1)))\n",
    "\n",
    "###############################################################################\n",
    "# GENERATE GRAPH\n",
    "###############################################################################\n",
    "\n",
    "# Get Hessian\n",
    "@tf.function\n",
    "def get_hessian(model, x):\n",
    "    with tf.GradientTape() as t2:\n",
    "        t2.watch(x)\n",
    "        with tf.GradientTape() as t1:\n",
    "            t1.watch(x)\n",
    "            y = model(x, training=False)\n",
    "            obj=tf.math.log(y+1E-15)\n",
    "        g=t1.gradient(obj,x)\n",
    "    return t2.jacobian(g,x)\n",
    "\n",
    "# Compute Superfeatures\n",
    "H=np.zeros([d,d])\n",
    "print('Computing Hessian...')\n",
    "t=time.time()\n",
    "for i in range(N_hess):\n",
    "    print('Sample '+str(i+1)+'/'+str(N_hess))\n",
    "    sample=x_hess[i:(i+1)]\n",
    "    hessian = get_hessian(teacher, sample)\n",
    "    H+=tf.reduce_mean(hessian, axis=[0,2]).numpy()\n",
    "elapsed_time=time.time()-t\n",
    "print(elapsed_time)\n",
    "\n",
    "H/=N_hess\n",
    "\n",
    "# Set up graph structure\n",
    "W_dir=np.abs(H)\n",
    "W=W_dir+W_dir.T\n",
    "np.fill_diagonal(W,0)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# COMPUTE SUPERFEATURE\n",
    "###############################################################################\n",
    "\n",
    "def compute_sf(W,M):\n",
    "    print('Detecting Communities...')\n",
    "    from sknetwork.clustering import Louvain\n",
    "    resolution=1.0\n",
    "    max_iter=100\n",
    "    n_com=1\n",
    "    count=0\n",
    "    while(n_com!=M):\n",
    "        louvain=Louvain(resolution = resolution, random_state=100)\n",
    "        parts = louvain.fit_transform(W)\n",
    "        n_com=np.max(parts)+1\n",
    "        print('No. of Communities: '+str(n_com))\n",
    "        if(n_com<M):\n",
    "            resolution=resolution+0.01\n",
    "        if(n_com>M):\n",
    "            resolution=resolution-0.01\n",
    "        count+=1\n",
    "        if(count>=max_iter):\n",
    "            raise ValueError('Number of superfeatures should be equal to the number of communities detected.')\n",
    "    print(resolution)\n",
    "    sf_idx=[]\n",
    "    for i in range(M):\n",
    "        sf_idx.append(list(np.where(np.array(parts)==i)[0]))\n",
    "    print(sf_idx)\n",
    "    return sf_idx\n",
    "\n",
    "sf_idx=compute_sf(W, M)\n",
    "# size=int(d/M)\n",
    "# np.random.seed(2000)\n",
    "# arr=np.random.permutation(d)\n",
    "# sf_idx=[arr[i*size:(i+1)*size] for i in range(M-1)]\n",
    "# sf_idx.append(arr[(M-1)*size:])\n",
    "#sf_idx=[np.arange(784), np.arange(784,d)]\n",
    "\n",
    "base_val=np.mean(teacher(x_train).numpy(), axis=0)\n",
    "base_logit=np.log(base_val)\n",
    "\n",
    "###############################################################################\n",
    "# EXPLAINING TEACHER\n",
    "###############################################################################\n",
    "n_params=teacher.count_params() # Teacher does not have any non-trainable param\n",
    "hlu_new_teacher=solve(M*(n_hl-1), M*(n_hl+num_of_classes)+d, M*num_of_classes-n_params)\n",
    "\n",
    "new_teacher=model_exp_teacher(d,num_of_classes,sf_idx)\n",
    "new_teacher.fit(x_train, y_train, epochs=t_epoch, batch_size=t_batch, verbose=train_verbose)\n",
    "\n",
    "y_pred_test=new_teacher.predict(x_test)\n",
    "\n",
    "print('Explaining Teacher Test Classification')\n",
    "print(confusion_matrix(y_test, np.argmax(y_pred_test,1)))\n",
    "print(bootstrap_score(y_test, np.argmax(y_pred_test,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "np.random.seed(1)\n",
    "rn.seed(2)\n",
    "tf.random.set_seed(3)\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder,  LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, accuracy_score\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import categorical_crossentropy, kl_divergence\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Dropout, Activation, Add, Multiply, Concatenate, LeakyReLU\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "d=784*2\n",
    "num_of_classes=100\n",
    "\n",
    "n_hl=2 # Number of hidden layers in student model\n",
    "hlu_student=20\n",
    "\n",
    "# Superfeatures\n",
    "M=2\n",
    "\n",
    "# Distillation params\n",
    "N_train=x_train.shape[0]\n",
    "s_epoch=100\n",
    "s_batch=100\n",
    "T=10\n",
    "tau=10\n",
    "L=0.7\n",
    "mu=0.7\n",
    "\n",
    "chunk_size=1000000\n",
    "chi_batch=100\n",
    "\n",
    "## Verbose\n",
    "train_verbose = 1\n",
    "\n",
    "###############################################################################\n",
    "# FUNCTION DEFINITION\n",
    "###############################################################################\n",
    "def set_seed_TF2(seed):\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    \n",
    "def custom_T(x,T=T):\n",
    "    return tf.math.softmax(tf.math.log(x+1E-15)/T)\n",
    "\n",
    "def random_order_cartesian_product(*factors, n_samples=100):\n",
    "    amount = functools.reduce(lambda prod, factor: prod * len(factor), factors, 1)\n",
    "    index_list = set([])\n",
    "    seed=0\n",
    "    while(len(index_list)!=n_samples):\n",
    "        seed+=1\n",
    "        rn.seed(seed)\n",
    "        index=rn.randint(0, amount)\n",
    "        index_list.add(index)\n",
    "    for index in index_list:\n",
    "        items = []\n",
    "        for factor in factors:\n",
    "            items.append(factor[index % len(factor)])\n",
    "            index //= len(factor)\n",
    "        yield items \n",
    "\n",
    "def model_nn_student(d, num_of_classes):\n",
    "    set_seed_TF2(100)\n",
    "    inp=Input(shape=(d,))\n",
    "    x=Dense(hlu_student, activation='relu')(inp)\n",
    "    for _ in range(n_hl-1):\n",
    "        x=Dense(hlu_student, activation='relu')(x)\n",
    "    tot_logit=Dense(num_of_classes)(x)\n",
    "    op=Activation('softmax')(tot_logit)\n",
    "    nn = Model(inputs=inp, outputs=op)\n",
    "    nn.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #nn.summary()\n",
    "    return nn\n",
    "\n",
    "def model_nn_soft(d, num_of_classes):\n",
    "    set_seed_TF2(100)\n",
    "    inp=Input(shape=(d,))\n",
    "    x=Dense(hlu_student, activation='relu')(inp)\n",
    "    for _ in range(n_hl-1):\n",
    "        x=Dense(hlu_student, activation='relu')(x)\n",
    "    tot_logit=Dense(num_of_classes)(x)\n",
    "    hard_softmax=Activation('softmax')(tot_logit)\n",
    "    soft_softmax=Activation('softmax')(tot_logit/T)\n",
    "    nn = Model(inputs=inp, outputs=[hard_softmax, soft_softmax])\n",
    "    nn.compile('adam', loss=['categorical_crossentropy', 'kl_divergence'], loss_weights=[1-L, T*T*L], metrics=['accuracy'])\n",
    "    #nn.summary()\n",
    "    return nn\n",
    "\n",
    "def model_nn_ked(d, num_of_classes, sf_idx):\n",
    "    set_seed_TF2(100)\n",
    "    inp=Input(shape=(d,))\n",
    "    logits=[]\n",
    "    ops=[]\n",
    "    loss=['categorical_crossentropy', 'kl_divergence']\n",
    "    loss_weights=[1-L, T*T*L*(1-mu)]\n",
    "    for i in range(M):\n",
    "        x=Dense(hlu_ked, activation='relu')(tf.gather(inp, sf_idx[i], axis=1))\n",
    "        for _ in range(n_hl-1):\n",
    "            x=Dense(hlu_ked, activation='relu')(x)\n",
    "        y=Dense(num_of_classes, activation='softmax', name='sf_'+str(i+1))(x)\n",
    "        logits.append(tf.math.log(y+1E-15))\n",
    "        ops.append(custom_T(y, T=tau))\n",
    "        loss.append('kl_divergence')\n",
    "        loss_weights.append(tau*tau*L*mu/M)\n",
    "    tot_logit=Add()(logits)-(M-1)*base_logit \n",
    "    soft_softmax=Activation('softmax')(tot_logit/T)\n",
    "    ops.insert(0,soft_softmax)\n",
    "    hard_softmax=Activation('softmax')(tot_logit)\n",
    "    ops.insert(0,hard_softmax)\n",
    "    nn = Model(inputs=inp, outputs=ops)\n",
    "    nn.compile('adam', loss=loss, loss_weights=loss_weights, metrics=['accuracy'])\n",
    "    #nn.summary()\n",
    "    return nn\n",
    "\n",
    "def solve(a, b, c):\n",
    "    if(b*b-4*a*c)<0:\n",
    "        raise ValueError('Problem is not feasible.')\n",
    "    sol=np.max(np.roots([a,b,c]))\n",
    "    return int(sol)\n",
    "\n",
    "def bootstrap_score(y_test, y_pred, metric=accuracy_score, l=0.95, seed=100):\n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    idx = np.arange(y_test.shape[0])\n",
    "    test_accuracies = []\n",
    "    for i in range(200):\n",
    "        pred_idx = rng.choice(idx, size=idx.shape[0], replace=True)\n",
    "        acc_test_boot = metric(y_test[pred_idx], y_pred[pred_idx])\n",
    "        test_accuracies.append(acc_test_boot)\n",
    "    bootstrap_score_mean = np.mean(test_accuracies)\n",
    "    [ci_lower, ci_upper] = confidence_interval(test_accuracies,l)\n",
    "    return bootstrap_score_mean, 0.5*(ci_upper-ci_lower)\n",
    "\n",
    "###############################################################################\n",
    "# EXPERIMENT\n",
    "###############################################################################\n",
    "\n",
    "# Dataset Preprocessing\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train_1, y_train_1), (x_test_1, y_test_1)=mnist.load_data()\n",
    "\n",
    "x_train_1=x_train_1.astype(np.float32).reshape(-1,784)\n",
    "x_test_1=x_test_1.astype(np.float32).reshape(-1,784)\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train_2, y_train_2), (x_test_2, y_test_2)=fashion_mnist.load_data()\n",
    "\n",
    "x_train_2=x_train_2.astype(np.float32).reshape(-1,784)\n",
    "x_test_2=x_test_2.astype(np.float32).reshape(-1,784)\n",
    "\n",
    "\n",
    "x_train=np.concatenate([x_train_1,x_train_2], axis=1)\n",
    "x_test=np.concatenate([x_test_1,x_test_2], axis=1)\n",
    "alpha_le=LabelEncoder()\n",
    "alpha_le.fit(['A','B','C','D','E','F','G','H','I','J'])\n",
    "alpha_train_2=alpha_le.inverse_transform(y_train_2)\n",
    "alpha_test_2=alpha_le.inverse_transform(y_test_2)\n",
    "y_train=[a+b for a,b in zip(y_train_1.astype(str),alpha_train_2)]\n",
    "y_test=[a+b for a,b in zip(y_test_1.astype(str),alpha_test_2)]\n",
    "\n",
    "le=LabelEncoder()\n",
    "y_train=le.fit_transform(y_train)\n",
    "y_test=le.transform(y_test)\n",
    "\n",
    "# Scaling\n",
    "#scl=StandardScaler()\n",
    "#x_train=scl.fit_transform(x_train)\n",
    "#x_test=scl.transform(x_test)\n",
    "x_train=x_train/255.0\n",
    "x_test=x_test/255.0\n",
    "\n",
    "y_train=y_train.reshape(-1,1)\n",
    "y_test=y_test.reshape(-1,1)\n",
    "\n",
    "# One-hot Encoding\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y_train=enc.fit_transform(y_train)\n",
    "\n",
    "base_val=np.mean(teacher(x_train).numpy(), axis=0)\n",
    "base_logit=np.log(base_val)\n",
    "\n",
    "regular=[]\n",
    "distilled_soft=[]\n",
    "distilled_ked=[]\n",
    "distilled_chi=[]\n",
    "baseline=[]\n",
    "\n",
    "idx=np.random.permutation(x_train.shape[0])[0:N_train]\n",
    "x_tr=x_train[idx]\n",
    "y_tr=y_train[idx]\n",
    "\n",
    "# Soft Labels\n",
    "soften = Model(inputs=teacher.inputs, outputs=[layer.output for layer in teacher.layers])\n",
    "y_soft = softmax(soften(x_tr)[-2]/T)\n",
    "\n",
    "###############################################################################\n",
    "# STUDENT\n",
    "###############################################################################\n",
    "student=model_nn_student(d,num_of_classes)\n",
    "\n",
    "student.fit(x_tr, y_tr, epochs=s_epoch, batch_size=s_batch, verbose=train_verbose)\n",
    "regular.append(bootstrap_score(y_test, np.argmax(student.predict(x_test),1)))\n",
    "\n",
    "print('Regular Test Classification')\n",
    "print(confusion_matrix(y_test, np.argmax(student.predict(x_test),1)))\n",
    "print(bootstrap_score(y_test, np.argmax(student.predict(x_test),1)))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# SOFT DISTILLATION\n",
    "###############################################################################\n",
    "student_soft=model_nn_soft(d, num_of_classes)\n",
    "\n",
    "student_soft.fit(x_tr, [y_tr, y_soft], epochs=s_epoch, batch_size=s_batch, verbose=train_verbose)\n",
    "y_pred_test=student_soft.predict(x_test)[0]\n",
    "distilled_soft.append(bootstrap_score(y_test, np.argmax(y_pred_test,1)))\n",
    "\n",
    "print('Distilled Test Classification (Soft)')\n",
    "print(confusion_matrix(y_test, np.argmax(y_pred_test, 1)))\n",
    "print(bootstrap_score(y_test, np.argmax(y_pred_test,1)))    \n",
    "\n",
    "###############################################################################\n",
    "# KNOWLEDGE EXPLAINING DISTILLATION\n",
    "###############################################################################\n",
    "train_labels=[y_tr, custom_T(new_teacher(x_tr))]\n",
    "for i in range(M):\n",
    "    soften_sf = Model(inputs=new_teacher.inputs, outputs=new_teacher.get_layer('sf_'+str(i+1)).output)\n",
    "    y_sf=soften_sf(x_tr).numpy()\n",
    "    train_labels.append(custom_T(y_sf, T=tau))\n",
    "    \n",
    "n_params=student.count_params() # Student does not have any non-trainable param\n",
    "hlu_ked=solve(M*(n_hl-1), M*(n_hl+num_of_classes)+d, M*num_of_classes-n_params)\n",
    "    \n",
    "student_ked=model_nn_ked(d, num_of_classes, sf_idx)\n",
    "\n",
    "student_ked.fit(x_tr, train_labels, epochs=s_epoch, batch_size=s_batch, verbose=train_verbose)\n",
    "y_pred_test=student_ked.predict(x_test)[0]\n",
    "distilled_ked.append(bootstrap_score(y_test, np.argmax(y_pred_test,1)))\n",
    "\n",
    "print('Distilled Test Classification (KED)')\n",
    "print(confusion_matrix(y_test, np.argmax(y_pred_test, 1)))\n",
    "print(bootstrap_score(y_test, np.argmax(y_pred_test,1)))\n",
    "\n",
    "###############################################################################\n",
    "# DISTILLATION USING CHIMERIC SET\n",
    "###############################################################################\n",
    "# Student's chimeric set\n",
    "sample_lists=[]\n",
    "for i in range(M):\n",
    "    sample_lists.append(x_tr[:,sf_idx[i]])\n",
    "gen=random_order_cartesian_product(*sample_lists, n_samples=chunk_size)\n",
    "\n",
    "chunk_list=[]\n",
    "# M-fold Cartesian product\n",
    "t=time.time()\n",
    "for element in gen:\n",
    "    jumbled=np.hstack(element)\n",
    "    ordered=jumbled[np.argsort(np.hstack(sf_idx))]\n",
    "    chunk_list.append(ordered)\n",
    "elapsed_time=time.time()-t\n",
    "print(elapsed_time)\n",
    "\n",
    "x_chi=np.asarray(chunk_list)\n",
    "y_chi=new_teacher(x_chi)\n",
    "\n",
    "train_labels=[enc.transform(np.argmax(y_chi,1).reshape(-1,1)), custom_T(y_chi)]\n",
    "for i in range(M):\n",
    "    soften_sf = Model(inputs=new_teacher.inputs, outputs=new_teacher.get_layer('sf_'+str(i+1)).output)\n",
    "    y_sf=soften_sf(x_chi).numpy()\n",
    "    train_labels.append(custom_T(y_sf, T=tau))\n",
    "\n",
    "student_chi=student_ked\n",
    "\n",
    "student_chi.fit(x_chi, train_labels, batch_size=chi_batch, verbose=train_verbose)\n",
    "y_pred_test=student_chi.predict(x_test)[0]\n",
    "distilled_chi.append(bootstrap_score(y_test, np.argmax(y_pred_test,1)))\n",
    "    \n",
    "print('Distilled Test Classification (Chimeric)')\n",
    "print(confusion_matrix(y_test, np.argmax(y_pred_test, 1)))\n",
    "print(bootstrap_score(y_test, np.argmax(y_pred_test,1)))\n",
    "    \n",
    "###############################################################################\n",
    "# BASELINE\n",
    "###############################################################################\n",
    "x_base=deepcopy(x_chi)\n",
    "y_base=teacher(x_base)\n",
    "y_base_soft=custom_T(y_base)\n",
    "    \n",
    "student_base=student_soft\n",
    "\n",
    "student_base.fit(x_base, [y_base, y_base_soft], batch_size=chi_batch, verbose=train_verbose)\n",
    "y_pred_test=student_base.predict(x_test)[0]\n",
    "baseline.append(bootstrap_score(y_test, np.argmax(y_pred_test,1)))\n",
    "    \n",
    "print('Distilled Test Classification (Baseline)')\n",
    "print(confusion_matrix(y_test, np.argmax(y_pred_test, 1)))\n",
    "print(bootstrap_score(y_test, np.argmax(y_pred_test,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as pil\n",
    "###############################################################################\n",
    "# VISUALIZE SUPERFEATURES\n",
    "###############################################################################\n",
    "i=100\n",
    "img = pil.fromarray(np.uint8(x_train[i].reshape(56,28) * 255) , 'L')\n",
    "img.show()\n",
    "sf_1=deepcopy(x_train[i])\n",
    "sf_1[sf_idx[1]]=0\n",
    "img_1 = pil.fromarray(np.uint8(sf_1.reshape(56,28) * 255) , 'L')\n",
    "img_1.show()\n",
    "sf_2=deepcopy(x_train[i])\n",
    "sf_2[sf_idx[0]]=0\n",
    "img_2 = pil.fromarray(np.uint8(sf_2.reshape(56,28) * 255) , 'L')\n",
    "img_2.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
