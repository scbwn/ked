import numpy as np
import tensorflow as tf
import random as rn
np.random.seed(1)
rn.seed(2)
tf.random.set_seed(3)
import os
os.chdir("./env/KED/main/")

import sys
print(sys.argv)

from sklearn.preprocessing import StandardScaler, OneHotEncoder,  LabelEncoder
from sklearn.utils import resample
from sklearn.metrics import confusion_matrix, balanced_accuracy_score, accuracy_score
from tensorflow.keras import backend as K
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.losses import categorical_crossentropy, kl_divergence, mean_squared_error
from tensorflow.keras.layers import Layer, Input, Dense, Dropout, BatchNormalization, Activation, Add, Multiply, Lambda 
from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Flatten, UpSampling2D
from tensorflow.keras import regularizers
from tensorflow.keras.layers import Resizing, RandomCrop, RandomFlip
from tensorflow.keras.activations import softmax
from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam
from copy import deepcopy
import time

dataset_name=sys.argv[1]
t_name=sys.argv[2]
s_name=sys.argv[3]

from tensorflow.keras.models import load_model
teacher=load_model('./models/'+dataset_name+'-'+t_name+'/teacher.h5')
new_teacher=load_model('./models/'+dataset_name+'-'+t_name+'/new_teacher.h5')

if(dataset_name=='cifar10'):
    d=(32,32,3)
    num_of_classes=10
    s_epoch=150
    s_batch=100
    decay_const=5E-4
    init_lr=0.05
elif(dataset_name=='cifar100'):
    d=(32,32,3)
    num_of_classes=100
    s_epoch=150
    s_batch=100
    decay_const=5E-4
    init_lr=0.05
elif(dataset_name=='imagenet'):
    d=(64,64,3)
    num_of_classes=200
    s_epoch=75
    s_batch=100
    decay_const=1E-4
    init_lr=0.1


if(s_name=='resnet8'):
    n=8
    k=1
    filters_per_branch=26
elif(s_name=='resnet20'):
    n=20
    k=1
    filters_per_branch=30
    
    

# Superfeatures
M=4

# Distillation params
T=10
tau=10
L=0.7
mu=0.7
rho=0.7

## Verbose
train_verbose = 2

###############################################################################
# FUNCTION DEFINITION
###############################################################################
def set_seed_TF2(seed):
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    rn.seed(seed)
    
def custom_T(x,T=T):
    return tf.math.softmax(tf.math.log(x+1E-15)/T)

def random_crop(img, random_crop_size):
    # Note: image_data_format is 'channel_last'
    # SOURCE: https://jkjung-avt.github.io/keras-image-cropping/
    assert img.shape[2] == 3
    height, width = img.shape[0], img.shape[1]
    dy, dx = random_crop_size
    x = np.random.randint(0, width - dx + 1)
    y = np.random.randint(0, height - dy + 1)
    return img[y:(y+dy), x:(x+dx), :]


def crop_generator(batches, crop_length, predictor=None):
    """Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator.
    SOURCE: https://jkjung-avt.github.io/keras-image-cropping/
    """
    while True:
        batch_x, batch_y = next(batches)
        if predictor is not None:
            train_labels=[batch_y]
            train_labels=train_labels+predictor(batch_x)
        else: 
            train_labels=batch_y
        paddings = tf.constant([[0, 0,], [4, 4], [4, 4], [0, 0]])
        batch_x = tf.pad(batch_x, paddings, mode="CONSTANT")
        batch_x = RandomFlip(mode="horizontal")(batch_x)
        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))
        for i in range(batch_x.shape[0]):
            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))
        yield (batch_crops, train_labels)

def resnet_layer(x,
                 num_filters,
                 kernel_size=3,
                 strides=1,
                 activation='relu',
                 batch_normalization=True):

    conv = Conv2D(num_filters,
                  kernel_size=kernel_size,
                  strides=strides,
                  padding='same',
                  kernel_initializer='he_normal',
                  kernel_regularizer=regularizers.l2(decay_const))
    x = conv(x)
    if batch_normalization:
        x = BatchNormalization()(x)
    if activation is not None:
        x = Activation(activation)(x)
    return x

def shared_layers(x, filters=16*k, depth=n):
    if (depth - 2) % 6 != 0:
        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')
    num_filters = filters
    num_res_blocks = int((depth - 2) / 6)

    x = resnet_layer(x, num_filters=num_filters)
    for stack in range(2):
        for res_block in range(num_res_blocks):
            strides = 1
            if stack > 0 and res_block == 0:
                strides = 2  # downsample
            y = resnet_layer(x,
                             num_filters=num_filters,
                             strides=strides)
            y = resnet_layer(y,
                             num_filters=num_filters,
                             activation=None)
            if stack > 0 and res_block == 0:  # first layer but not first stack
                pad_dim=num_filters-x.shape[-1]
                paddings=tf.constant([[0, 0,], [0, 0], [0, 0], [pad_dim-pad_dim//2, pad_dim//2]])
                x = tf.pad(x[:, ::2, ::2, :], paddings, mode="CONSTANT")
            x = Add()([x, y])
            x = Activation('relu')(x)
        num_filters*=2
    return x
    
def branch_layers(x, filters=64*k, depth=n):
    if (depth - 2) % 6 != 0:
        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')
    num_filters = filters
    num_res_blocks = int((depth - 2) / 6)
    
    for stack in range(2,3):
        for res_block in range(num_res_blocks):
            strides = 1
            if stack > 0 and res_block == 0:
                strides = 2  # downsample
            y = resnet_layer(x,
                             num_filters=num_filters,
                             strides=strides)
            y = resnet_layer(y,
                             num_filters=num_filters,
                             activation=None)
            if stack > 0 and res_block == 0:  # first layer but not first stack
                pad_dim=num_filters-x.shape[-1]
                paddings=tf.constant([[0, 0,], [0, 0], [0, 0], [pad_dim-pad_dim//2, pad_dim//2]])
                x = tf.pad(x[:, ::2, ::2, :], paddings, mode="CONSTANT")
            x = Add()([x, y])
            x = Activation('relu')(x)
        num_filters*=2
    return x

def model_nn_student(d, num_of_classes):
    set_seed_TF2(100)
    inp=Input(shape=d)
    shared_op = shared_layers(inp)
    branch_op=branch_layers(shared_op)
    flat_inp = GlobalAveragePooling2D()(branch_op)
    op=Dense(num_of_classes, activation='softmax')(flat_inp)
    nn = Model(inputs=inp, outputs=op)
    nn.compile(optimizer=SGD(momentum=0.9, nesterov=True), loss='categorical_crossentropy', metrics=['accuracy'])
    return nn

def model_nn_soft(d, num_of_classes):
    set_seed_TF2(100)
    inp=Input(shape=d)
    shared_op = shared_layers(inp)
    branch_op=branch_layers(shared_op)
    proj_op = branch_op
    flat_inp = GlobalAveragePooling2D()(branch_op)
    tot_logit=Dense(num_of_classes)(flat_inp)
    hard_softmax=Activation('softmax')(tot_logit)
    soft_softmax=Activation('softmax')(tot_logit/T)
    nn = Model(inputs=inp, outputs=[hard_softmax, soft_softmax, proj_op])
    nn.compile(SGD(momentum=0.9, nesterov=True), loss=['categorical_crossentropy', 'kl_divergence', custom_loss], 
               loss_weights=[1-L, T*T*L*(1-rho), L*rho], metrics=['accuracy'])
    #nn.summary()
    return nn

def model_nn_ked(d, num_of_classes):
    set_seed_TF2(100)
    inp=Input(shape=d)
    shared_op = shared_layers(inp)
    shared_op = resnet_layer(shared_op, num_filters=M*filters_per_branch, kernel_size=1)
    shared_op_list=tf.split(shared_op, M, axis=-1)
    logits=[]
    proj_op_list=[]
    ops=[]
    loss=['categorical_crossentropy', 'kl_divergence']
    loss_weights=[1-L, T*T*L*(1-mu)]
    for i in range(M):
        branch_op=branch_layers(shared_op_list[i], filters=filters_per_branch)
        proj_op_list.append(branch_op)
        flat_inp = GlobalAveragePooling2D()(branch_op)
        y=Dense(num_of_classes, activation='softmax', name='sf_'+str(i+1))(flat_inp)
        logits.append(tf.math.log(y+1E-15))
        ops.append(custom_T(y, T=tau))
        loss.append('kl_divergence')
        loss_weights.append(tau*tau*L*mu*(1-rho)/M)
    ops=ops+proj_op_list
    loss=loss+M*[custom_loss]
    loss_weights=loss_weights+M*[L*mu*rho/M]
    tot_logit=Add()(logits)
    soft_softmax=Activation('softmax')(tot_logit/T)
    ops.insert(0,soft_softmax)
    hard_softmax=Activation('softmax')(tot_logit)
    ops.insert(0,hard_softmax)
    nn = Model(inputs=inp, outputs=ops)
    nn.compile(SGD(momentum=0.9, nesterov=True), loss=loss, loss_weights=loss_weights, metrics=['accuracy'])
    #nn.summary()
    return nn

def custom_loss(y_true, y_pred):
    q_true=Flatten()(y_true)
    q_pred=Flatten()(y_pred)
    g_true=tf.matmul(q_true, q_true, transpose_b=True)
    g_pred=tf.matmul(q_pred, q_pred, transpose_b=True)
    return 3000*mean_squared_error(g_true/(tf.norm(g_true, ord='euclidean', axis=1, keepdims=True)+1E-15), 
                           g_pred/(tf.norm(g_pred, ord='euclidean', axis=1, keepdims=True)+1E-15))

def lr_scheduler(epoch):
    lr=0.01
    if epoch>=10:
        lr=init_lr
    if epoch>=int(s_epoch*12/15):
        lr=lr/10
    if epoch>=int(s_epoch*14/15):
        lr=lr/10
    return lr

def solve(a, b, c):
    if(b*b-4*a*c)<0:
        raise ValueError('Problem is not feasible.')
    sol=np.max(np.roots([a,b,c]))
    return int(sol)

def confidence_interval(a,l):
    import numpy as np, scipy.stats as st
    return st.t.interval(l, len(a)-1, loc=np.mean(a), scale=st.sem(a))

def bootstrap_score(y_test, y_pred, metric=accuracy_score, l=0.95, seed=100):
    rng = np.random.RandomState(seed=seed)
    idx = np.arange(y_test.shape[0])
    test_accuracies = []
    for i in range(200):
        pred_idx = rng.choice(idx, size=idx.shape[0], replace=True)
        acc_test_boot = metric(y_test[pred_idx], y_pred[pred_idx])
        test_accuracies.append(acc_test_boot)
    bootstrap_score_mean = np.mean(test_accuracies)
    [ci_lower, ci_upper] = confidence_interval(test_accuracies,l)
    return bootstrap_score_mean, 0.5*(ci_upper-ci_lower)

###############################################################################
# EXPERIMENT
###############################################################################

with tf.device("CPU"):
    # Dataset Preprocessing
    if(dataset_name=='cifar10'):
        from tensorflow.keras.datasets import cifar10
        (x_train, y_train), (x_test, y_test)=cifar10.load_data()
    elif(dataset_name=='cifar100'):
        from tensorflow.keras.datasets import cifar100
        (x_train, y_train), (x_test, y_test)=cifar100.load_data()
    elif(dataset_name=='imagenet'):
        x_train=np.load('./tiny imagenet/train_data.npy', allow_pickle=True)
        y_train=np.load('./tiny imagenet/train_labels.npy', allow_pickle=True)
        x_test=np.load('./tiny imagenet/test_data.npy', allow_pickle=True)
        y_test=np.load('./tiny imagenet/test_labels.npy', allow_pickle=True)
    
    x_train=x_train.astype(np.float32)/255
    x_test=x_test.astype(np.float32)/255
    
    # Scaling
    mean=x_train.mean((0,1,2))
    std=x_train.std((0,1,2))
    x_train=(x_train-mean)/std
    x_test=(x_test-mean)/std
    
    y_train=y_train.reshape(-1,1)
    y_test=y_test.reshape(-1,1)
    
    
    # One-hot Encoding
    enc = OneHotEncoder(sparse=False)
    y_train=enc.fit_transform(y_train)
    
    
    # Data generator for training data
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    train_generator = ImageDataGenerator()
    
    # Generate training batches
    train_batches = train_generator.flow(x_train, y_train, batch_size=s_batch)
    train_batches = crop_generator(train_batches, d[0])

callbacks = [tf.keras.callbacks.LearningRateScheduler(lr_scheduler)]

###############################################################################
# STUDENT
###############################################################################
# student=model_nn_student(d,num_of_classes)
# student.fit(train_batches, steps_per_epoch=x_train.shape[0]//s_batch, epochs=s_epoch, callbacks=callbacks, verbose=train_verbose)
# with tf.device("CPU"):
#     y_pred_test=student.predict(x_test)

# print('Regular Test Classification')
# print(confusion_matrix(y_test, np.argmax(y_pred_test,1)))
# print(bootstrap_score(y_test, np.argmax(y_pred_test,1)))

###############################################################################
# KNOWLEDGE DISTILLATION
###############################################################################
outputs=[custom_T(teacher.output), teacher.get_layer('branch_op').output]
proj_dim = teacher.get_layer('branch_op').output.shape[3]
predictor = Model(inputs=teacher.inputs, outputs=outputs)

# Generate training batches
train_batches = train_generator.flow(x_train, y_train, batch_size=s_batch)
train_batches = crop_generator(train_batches, d[0], predictor=predictor)

student_soft=model_nn_soft(d, num_of_classes)
student_soft.fit(train_batches, steps_per_epoch=x_train.shape[0]//s_batch, epochs=s_epoch, callbacks=callbacks, verbose=train_verbose)
with tf.device("CPU"):
    y_pred_test=student_soft.predict(x_test)[0]

print('Distilled Test Classification (KD)')
print(confusion_matrix(y_test, np.argmax(y_pred_test, 1)))
print(bootstrap_score(y_test, np.argmax(y_pred_test,1)))

###############################################################################
# KNOWLEDGE EXPLANATION DISTILLATION
###############################################################################
outputs=[custom_T(new_teacher.output)]
proj_op=[]
new_proj_dim=[]
import gc
for i in range(M):
    gc.collect()
    outputs.append(custom_T(new_teacher.get_layer('sf_'+str(i+1)).output, T=tau))
    proj_op.append(new_teacher.get_layer('branch_op_'+str(i+1)).output)
    new_proj_dim.append(new_teacher.get_layer('branch_op_'+str(i+1)).output.shape[3])
outputs=outputs+proj_op
predictor= Model(inputs=new_teacher.inputs, outputs=outputs)

# Generate training batches
train_batches = train_generator.flow(x_train, y_train, batch_size=s_batch)
train_batches = crop_generator(train_batches, d[0], predictor=predictor)

student_ked=model_nn_ked(d, num_of_classes)
student_ked.fit(train_batches, steps_per_epoch=x_train.shape[0]//s_batch, epochs=s_epoch, callbacks=callbacks, verbose=train_verbose)
with tf.device("CPU"):
    y_pred_test=student_ked.predict(x_test)[0]

print('Distilled Test Classification (KED)')
print(confusion_matrix(y_test, np.argmax(y_pred_test, 1)))
print(bootstrap_score(y_test, np.argmax(y_pred_test,1)))